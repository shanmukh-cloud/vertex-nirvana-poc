{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'kfp<2' \\\n",
    "                        'google-cloud-pipeline-components<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0JrvuK6LUYnQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_versions"
   },
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check_versions:kfp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.22\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a47846030fef"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[core]\n",
      "account = 665554893092-compute@developer.gserviceaccount.com\n",
      "disable_usage_reporting = True\n",
      "project = vertex-nirvana-poc\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "! gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3c8049930470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"vertex-nirvana-poc\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a54f9d7c1876"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3aaadaaf9b30"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c0404984792"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fV-KyGAX4Xl"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://vertex-nirvana-poc\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = f\"gs://vertex-nirvana-poc/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Oz8J0vmSlugt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-nirvana-poc/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'vertex-nirvana-poc' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b556df542518"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"665554893092-compute@developer.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0QNsyzEF2Ou4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://vertex-nirvana-poc/\n",
      "No changes made to gs://vertex-nirvana-poc/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aip_constants:endpoint"
   },
   "source": [
    "#### Vertex AI constants\n",
    "\n",
    "Setup up the following constants for Vertex AI:\n",
    "\n",
    "- `API_ENDPOINT`: The Vertex AI API service endpoint for `Dataset`, `Model`, `Job`, `Pipeline` and `Endpoint` services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TnO8gVBb2Ou4"
   },
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_constants"
   },
   "source": [
    "#### Vertex AI Pipelines constants\n",
    "\n",
    "Setup up the following constants for Vertex AI Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ONHtQDz32Ou5"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mbpw7oyM2Ou5"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/intro\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform==1.20.0\",\n",
    "        \"pandas\",\n",
    "        \"fsspec\",\n",
    "        \"gcsfs\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pyarrow\",\n",
    "    ],\n",
    ")\n",
    "def data_download(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    feature_store: str,\n",
    "    data_url: str,\n",
    "    split_date: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform as aip\n",
    "\n",
    "    logging.warn(\"Import file:\", data_url)\n",
    "\n",
    "    # Initialize Vertex AI client\n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    # Initiate feature store and run batch serve request\n",
    "    flight_delays_feature_store = aip.Featurestore(featurestore_name=feature_store)\n",
    "\n",
    "    read_instances = pd.read_csv(data_url)\n",
    "    read_instances[\"flight\"] = read_instances[\"flight\"].astype(str)\n",
    "    read_instances[\"airport\"] = read_instances[\"airport\"].astype(str)\n",
    "    read_instances[\"timestamp\"] = pd.to_datetime(read_instances[\"timestamp\"])\n",
    "\n",
    "    # Read features into a dataframe\n",
    "    data: pd.DataFrame = flight_delays_feature_store.batch_serve_to_df(\n",
    "        serving_feature_ids={\n",
    "            \"flight\": [\"*\"],\n",
    "            \"airport\": [\"*\"],\n",
    "        },\n",
    "        read_instances_df=read_instances,\n",
    "    )\n",
    "\n",
    "    completed_flights = data[~data[\"is_cancelled\"]]\n",
    "\n",
    "    # Consider flights that arrive more than 15 min late as delayed\n",
    "    completed_flights[\"target\"] = completed_flights[\"arrival_delay_minutes\"] > 15\n",
    "    training_data = completed_flights.drop(\n",
    "        columns=[\n",
    "            \"timestamp\",\n",
    "            \"entity_type_flight\",\n",
    "            \"is_cancelled\",\n",
    "            \"arrival_delay_minutes\",\n",
    "            \"origin_airport_id\",\n",
    "            \"entity_type_airport\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_data = training_data[completed_flights[\"timestamp\"] >= split_date]\n",
    "    training_data = training_data[completed_flights[\"timestamp\"] < split_date]\n",
    "\n",
    "    training_data.to_csv(dataset_train.path, index=False)\n",
    "    test_data.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_train(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Artifact],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    }\n",
    "\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    import pathlib\n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def model_evaluate(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "\n",
    "    data = pd.read_csv(test_set.path)[:1000]\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data.target\n",
    "    y_pred = model_pipeline.predict(X)\n",
    "\n",
    "    y_scores = model_pipeline.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_scores, pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"False\", \"True\"],\n",
    "        confusion_matrix(y, y_pred).tolist(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/665554893092/locations/us-central1/pipelineJobs/gcp-mlops-v0-20231011171506\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/665554893092/locations/us-central1/pipelineJobs/gcp-mlops-v0-20231011171506')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/gcp-mlops-v0-20231011171506?project=665554893092\n",
      "PipelineJob projects/665554893092/locations/us-central1/pipelineJobs/gcp-mlops-v0-20231011171506 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/665554893092/locations/us-central1/pipelineJobs/gcp-mlops-v0-20231011171506 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/665554893092/locations/us-central1/pipelineJobs/gcp-mlops-v0-20231011171506 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/665554893092/locations/us-central1/pipelineJobs/gcp-mlops-v0-20231011171506 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [data-download].; Job (project_id = vertex-nirvana-poc, job_id = 5409011718716129280) is failed due to the above error.; Failed to handle the job: {project_number = 665554893092, job_id = 5409011718716129280}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 67\u001b[0m\n\u001b[1;32m     59\u001b[0m aip\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mPROJECT_ID, staging_bucket\u001b[38;5;241m=\u001b[39mBUCKET, location\u001b[38;5;241m=\u001b[39mREGION)\n\u001b[1;32m     61\u001b[0m job \u001b[38;5;241m=\u001b[39m aip\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m     62\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcp-mlops-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcp-mlops-v0.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     64\u001b[0m     pipeline_root\u001b[38;5;241m=\u001b[39mPIPELINE_ROOT,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSERVICE_ACCOUNT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:319\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:817\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    816\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    820\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:356\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    351\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    352\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m    353\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[1;32m    354\u001b[0m )\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:595\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    597\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [data-download].; Job (project_id = vertex-nirvana-poc, job_id = 5409011718716129280) is failed due to the above error.; Failed to handle the job: {project_number = 665554893092, job_id = 5409011718716129280}\"\n"
     ]
    }
   ],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@dsl.pipeline(name=\"gcp-mlops-v0\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    training_data_url: str = f\"gs://{BUCKET}/features/read_instances/-00000-of-00001.csv\",\n",
    "    test_split_date: str = \"2021-12-20\",\n",
    "):\n",
    "    data_op = data_download(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        feature_store=\"flight_delays\",\n",
    "        data_url=training_data_url,\n",
    "        split_date=test_split_date,\n",
    "    )\n",
    "\n",
    "    from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "        create_custom_training_job_op_from_component,\n",
    "    )\n",
    "\n",
    "    custom_job_distributed_training_op = create_custom_training_job_op_from_component(\n",
    "        model_train, replica_count=1\n",
    "    )\n",
    "\n",
    "    model_train_op = custom_job_distributed_training_op(\n",
    "        dataset=data_op.outputs[\"dataset_train\"],\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    model_evaluate_op = model_evaluate(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=model_train_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-model\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_evaluate_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-endpoint\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"flight-delay-model\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"gcp-mlops-v0.json\")\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET, location=REGION)\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"gcp-mlops-v0\",\n",
    "    template_path=\"gcp-mlops-v0.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:hello_world"
   },
   "source": [
    "### Define Python function-based pipeline components\n",
    "\n",
    "In this tutorial, you define a simple pipeline that has three steps, where each step is defined as a component.\n",
    "\n",
    "#### Define hello_world component\n",
    "\n",
    "First, define a component based on a very simple Python function. It takes a string input parameter and returns that value as output.\n",
    "\n",
    "Note the use of the `@component` decorator, which compiles the function to a KFP component when evaluated.  For example purposes, this example specifies a base image to use for the component (`python:3.9`), and a component YAML file, `hw.yaml`. The compiled component specification is written to this file.  (The default base image is `python:3.7`, which would of course work just fine too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component\n",
    "def conditional_branch_op(output: str) -> str:\n",
    "    \"\"\"Conditionally branch based on the output value.\"\"\"\n",
    "    # Define your condition logic here\n",
    "    if output == \"Hello, world!\":\n",
    "        return \"condition_met\"\n",
    "    else:\n",
    "        return \"condition_not_met\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/665554893092/locations/us-central1/pipelineJobs/intro-pipeline-unique-20231004155310\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/665554893092/locations/us-central1/pipelineJobs/intro-pipeline-unique-20231004155310')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/intro-pipeline-unique-20231004155310?project=665554893092\n",
      "PipelineJob run completed. Resource name: projects/665554893092/locations/us-central1/pipelineJobs/intro-pipeline-unique-20231004155310\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "from kfp import dsl, components\n",
    "\n",
    "# Define your component functions here\n",
    "\n",
    "@components.func_to_container_op\n",
    "# def hello_world(text: str) -> str:\n",
    "#     print(text)\n",
    "#     return text\n",
    "#@func_to_container_op\n",
    "def hello_world(text: str) -> str:\n",
    "    return f\"Hello, {text}!\"\n",
    "\n",
    "\n",
    "#     return step1.output , step2.output, step3.output, step4.output\n",
    "@dsl.pipeline(name='Conditional Hello World Pipeline')\n",
    "def conditional_hello_world_pipeline(input_condition: str) -> NamedTuple(\n",
    "    'Outputs',\n",
    "    [\n",
    "        ('step1_output', str),\n",
    "        ('step2_output', str),\n",
    "        ('step3_output', str),\n",
    "        ('step4_output', str),\n",
    "    ]\n",
    "):\n",
    "    # Step 1: Call the hello_world function with \"world\" input\n",
    "    step1 = hello_world(\"world\")\n",
    "\n",
    "    # Step 2: Use the input_condition parameter to control the branching\n",
    "    with dsl.Condition(input_condition == \"condition_met\"):\n",
    "        # If the input_condition is \"condition_met\", execute this branch\n",
    "        step2 = hello_world(\"Alice\")\n",
    "\n",
    "    with dsl.Condition(input_condition == \"condition_met\"):\n",
    "        # If the input_condition is \"condition_met\", execute this branch\n",
    "        step3 = hello_world(\"Bob\")\n",
    "\n",
    "    # Step 4: Always execute this step\n",
    "    step4 = hello_world(\"OpenAI\")\n",
    "\n",
    "    return step1.output, step2.output, step3.output, step4.output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"intro-pipeline-unique\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(text: str = \"hello world\"):\n",
    "    hw_task = hello_world(text)\n",
    "    # two_outputs_task = two_outputs(text)\n",
    "    conditional_hello_world_pipeline_task = conditional_hello_world_pipeline(hw_task.output)\n",
    "    # consumer_task = consumer(\n",
    "    #     hw_task.output,\n",
    "    #     two_outputs_task.outputs[\"output_one\"],\n",
    "    #     two_outputs_task.outputs[\"output_two\"],\n",
    "    # )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"Conditional_pipeline_job_with_caching.json\")\n",
    "    DISPLAY_NAME = \"Conditional_pipeline_job_with_caching\"\n",
    "    job = aip.PipelineJob(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        template_path=\"Conditional_pipeline_job_with_caching.json\",\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        enable_caching=True,\n",
    "    )\n",
    "    job.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWcIXuxR2Ou6"
   },
   "source": [
    "As you'll see below, compilation of this component creates a [task factory function](https://www.kubeflow.org/docs/components/pipelines/sdk/python-function-components/)—called `hello_world`— that you can use in defining a pipeline step.\n",
    "\n",
    "While not shown here, if you want to share this component definition, or use it in another context, you could also load it from its yaml file like this:\n",
    "`hello_world_op = components.load_component_from_file('./hw.yaml')`.\n",
    "You can also use the `load_component_from_url` method, if your component yaml file is stored online. (For GitHub URLs, load the 'raw' file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:two_outputs"
   },
   "source": [
    "#### Define two_outputs component\n",
    "\n",
    "The first component below, `two_outputs`, demonstrates installing a package -- in this case the `google-cloud-storage` package. Alternatively, you can specify a base image that includes the necessary installations.\n",
    "\n",
    "*Note:* The component function won't actually use the package.\n",
    "\n",
    "Alternatively, you can specify a base image that includes the necessary installations.\n",
    "\n",
    "The `two_outputs` component returns two named outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_component:consumer"
   },
   "source": [
    "#### Define the consumer component\n",
    "\n",
    "The third component, `consumer`, takes three string inputs and prints them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_pipeline:intro"
   },
   "source": [
    "### Define a pipeline that uses the components\n",
    "\n",
    "Next, define a pipeline that uses these three components.\n",
    "\n",
    "By evaluating the component definitions above, you've created task factory functions that are used in the pipeline definition to create the pipeline steps.\n",
    "\n",
    "The pipeline takes an input parameter, and passes that parameter as an argument to the first two pipeline steps (`hw_task` and `two_outputs_task`).\n",
    "\n",
    "Then, the third pipeline step (`consumer_task`) consumes the outputs of the first and second steps.  Because the `hello_world` component definition just returns one unnamed output, you refer to it as `hw_task.output`.  The `two_outputs` task returns two named outputs, which you access as `two_outputs_task.outputs[\"<output_name>\"]`.\n",
    "\n",
    "*Note:* In the `@dsl.pipeline` decorator, you're defining the `PIPELINE_ROOT` Cloud Storage path to use.  If you had not included that info here, it would be required to specify it when creating the pipeline run, as you'll see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:intro"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Next, run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_run:intro"
   },
   "source": [
    "Click on the generated link to see your run in the Cloud Console.\n",
    "\n",
    "<!-- It should look something like this as it is running:\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/automl_tabular_classif.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/automl_tabular_classif.png\" width=\"40%\"/></a> -->\n",
    "\n",
    "In the UI, many of the pipeline DAG nodes will expand or collapse when you click on them. Here is a partially-expanded view of the DAG (click image to see larger version).\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/intro_pipeline.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/intro_pipeline.png\" width=\"60%\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d428cc8803e7"
   },
   "source": [
    "### Delete the pipeline job\n",
    "\n",
    "You can delete the pipeline job with the method `delete()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "97802d9432e7"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline_service_account"
   },
   "source": [
    "## Specifying a service account to use for a pipeline run\n",
    "\n",
    "By default, the [service account](https://cloud.google.com/iam/docs/service-accounts) used for your pipeline run is your [default compute engine service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account).\n",
    "However, you might want to run pipelines with permissions to access different roles than those configured for your default SA (e.g. perhaps using a more restricted set of permissions).\n",
    "\n",
    "If you want to execute your pipeline using a different service account, this is straightforward to do.  You just need to give the new service account the correct permissions.\n",
    "\n",
    "### Create a service account\n",
    "\n",
    "Once your service account is configured, you pass it as an argument to the `create_run_from_job_spec` method. The pipeline job runs with the permissions of the given service account.\n",
    "\n",
    "Learn about [creating and configuring a service account to work with Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/configure-project#service-account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pandas import DataFrame\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def ingest_features_df(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    featurestore_id: str,\n",
    "    entity_type_id: str,\n",
    "    features_ids: List[str],\n",
    "    feature_time: str,\n",
    "    features_df: DataFrame,\n",
    "    entity_id_field: str\n",
    ") -> aiplatform.featurestore.EntityType:\n",
    "    \"\"\"\n",
    "    Ingests features into a Featurestore from a Pandas DataFrame.\n",
    "    Args:\n",
    "        project: The Google Cloud project ID.\n",
    "        location: The Google Cloud location.\n",
    "        featurestore_id: The Featurestore ID.\n",
    "        entity_type_id: The Entity Type ID.\n",
    "        features_ids: The list of Feature IDs.\n",
    "        feature_time: The Feature timestamp.\n",
    "        features_df: The Pandas DataFrame containing the features.\n",
    "        entity_id_field: The Entity ID field.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize the Vertex SDK for Python\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # Get the entity type from an existing Featurestore\n",
    "    entity_type = aiplatform.featurestore.EntityType(entity_type_id=entity_type_id,\n",
    "                                                     featurestore_id=featurestore_id)\n",
    "    # Ingest the features\n",
    "    entity_type.ingest_from_df(\n",
    "        feature_ids=features_ids,\n",
    "        feature_time=feature_time,\n",
    "        df_source=features_df,\n",
    "        entity_id_field=entity_id_field\n",
    "    )\n",
    "\n",
    "    return entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from pandas import DataFrame\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def batch_serve_features_df(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        featurestore_id: str,\n",
    "        serving_feature_ids: Dict[str, List[str]],\n",
    "        read_instances_df: DataFrame,\n",
    "        pass_through_fields: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves batch feature values from a Featurestore and writes them to a GCS bucket.\n",
    "    Args:\n",
    "        project: The Google Cloud project ID.\n",
    "        location: The Google Cloud location.\n",
    "        featurestore_id: The Featurestore ID.\n",
    "        serving_feature_ids: The dictionary of Entity Type IDs and Feature IDs to retrieve.\n",
    "        read_instances_df: The Pandas DataFrame containing entities and feature values.\n",
    "        pass_through_fields: The list of fields to pass through extra to the label column.\n",
    "    Returns:\n",
    "        The Pandas DataFrame containing the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Vertex SDK for Python\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # Get an existing Featurestore\n",
    "    featurestore = aiplatform.featurestore.Featurestore(featurestore_name=featurestore_id)\n",
    "\n",
    "    # Get data with a point-in-time query from the Featurestore\n",
    "    df = featurestore.batch_serve_to_df(\n",
    "        serving_feature_ids=serving_feature_ids,\n",
    "        read_instances_df=read_instances_df,\n",
    "        pass_through_fields=pass_through_fields\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3494/2227721041.py:1: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2.dsl import component\n",
      "/var/tmp/ipykernel_3494/2227721041.py:5: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(output_component_file=\"batch_serve_features_gcs.yaml\",\n",
      "/var/tmp/ipykernel_3494/2227721041.py:8: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def batch_serve_features_gcs(feature_store_id: str,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2.dsl import component\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "@component(output_component_file=\"batch_serve_features_gcs.yaml\",\n",
    "           base_image=\"python:3.9\",\n",
    "           packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def batch_serve_features_gcs(feature_store_id: str,\n",
    "                             gcs_destination_output_uri_prefix: str,\n",
    "                             gcs_destination_type: str,\n",
    "                             serving_feature_ids: str,\n",
    "                             read_instances_uri: str,\n",
    "                             project: str,\n",
    "                             location: str) -> NamedTuple(\"Outputs\", [(\"gcs_destination_output_uri_paths\", str)]):\n",
    "    # Import libraries\n",
    "    import os\n",
    "    from json import loads\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform.featurestore import Featurestore\n",
    "\n",
    "    # Initialize Vertex AI client\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # Initiate feature store and run batch serve request\n",
    "    featurestore = Featurestore(featurestore_name=feature_store_id)\n",
    "\n",
    "    # Serve features in batch on GCS\n",
    "    serving_feature_ids = loads(serving_feature_ids)\n",
    "    featurestore.batch_serve_to_gcs(\n",
    "        gcs_destination_output_uri_prefix=gcs_destination_output_uri_prefix,\n",
    "        gcs_destination_type=gcs_destination_type,\n",
    "        serving_feature_ids=serving_feature_ids,\n",
    "        read_instances_uri=read_instances_uri\n",
    "    )\n",
    "\n",
    "    # Store metadata\n",
    "    gcs_destination_output_path_prefix = gcs_destination_output_uri_prefix.replace(\"gcs://\", \"/gcs/\")\n",
    "    gcs_destination_output_paths = os.path.join(gcs_destination_output_path_prefix, \"*.csv\")\n",
    "    component_outputs = NamedTuple(\"Outputs\",\n",
    "                                   [(\"gcs_destination_output_uri_paths\", str), ], )\n",
    "\n",
    "    return component_outputs(gcs_destination_output_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pandas import DataFrame\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def online_serve_feature_values(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        featurestore_id: str,\n",
    "        entity_type_id: str,\n",
    "        entity_ids: List[str],\n",
    "        feature_ids: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves online feature values from a Featurestore.\n",
    "    Args:\n",
    "        project: The Google Cloud project ID.\n",
    "        location: The Google Cloud location.\n",
    "        featurestore_id: The Featurestore ID.\n",
    "        entity_type_id: The Entity Type ID.\n",
    "        entity_ids: The list of Entity IDs.\n",
    "        feature_ids: The list of Feature IDs.\n",
    "    Returns:\n",
    "        A Pandas DataFrame containing the feature values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Vertex SDK for Python\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    # Get the entity type from an existing Featurestore\n",
    "    entity_type = aiplatform.featurestore.EntityType(entity_type_id=entity_type_id,\n",
    "                                                     featurestore_id=featurestore_id)\n",
    "    # Retrieve the feature values\n",
    "    feature_values = entity_type.read(entity_ids=entity_ids, feature_ids=feature_ids)\n",
    "\n",
    "    return feature_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def batch_serve_features_to_bq_sample(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    featurestore_name: str,\n",
    "    bq_destination_output_uri: str,\n",
    "    read_instances_uri: str,\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    fs = aiplatform.featurestore.Featurestore(featurestore_name=featurestore_name)\n",
    "\n",
    "    SERVING_FEATURE_IDS = {\n",
    "        \"users\": [\"age\", \"gender\", \"liked_genres\",\"favorites\"],\n",
    "        \"movies\": [\"title\", \"average_rating\", \"genres\"],\n",
    "    }\n",
    "\n",
    "    fs.batch_serve_to_bq(\n",
    "        bq_destination_output_uri=bq_destination_output_uri,\n",
    "        serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "        read_instances_uri=read_instances_uri,\n",
    "        sync=sync,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROJECT_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mPROJECT_ID\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PROJECT_ID' is not defined"
     ]
    }
   ],
   "source": [
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acat.p2-665554893092-2def6259-1b87-46c7-8d5e-ece13075da57\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "! gcloud services enable compute.googleapis.com \\\n",
    "                       containerregistry.googleapis.com \\\n",
    "                       aiplatform.googleapis.com \\\n",
    "                       cloudbuild.googleapis.com \\\n",
    "                       cloudfunctions.googleapis.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-nirvana-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'vertex-nirvana-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -c STANDARD -l us-central1 gs://vertex-nirvana-bucket/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET= f\"gs://vertex-nirvana-bucket/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (\n",
    "    Dataset,\n",
    "    Output,\n",
    "    component,\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\",\n",
    ")\n",
    "def data_download(\n",
    "    data_url: str,\n",
    "    split_date: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    import logging\n",
    "\n",
    "    logging.warn(\"Import file:\", data_url)\n",
    "\n",
    "    data = pd.read_csv(data_url)\n",
    "\n",
    "    cancelled = (data[\"Cancelled\"] > 0) | (data[\"Diverted\"] > 0)\n",
    "    completed_flights = data[~cancelled]\n",
    "\n",
    "    training_data = completed_flights[[\"DepDelay\", \"TaxiOut\", \"Distance\"]]\n",
    "    # Consider flights that arrive more than 15 min late as delayed\n",
    "    training_data[\"target\"] = completed_flights[\"ArrDelay\"] > 15\n",
    "\n",
    "    test_data = training_data[completed_flights[\"FlightDate\"] >= split_date]\n",
    "    training_data = training_data[completed_flights[\"FlightDate\"] < split_date]\n",
    "\n",
    "    training_data.to_csv(dataset_train.path, index=False)\n",
    "    test_data.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Output,\n",
    "    component,\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\",\n",
    ")\n",
    "def model_train(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Artifact],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "    X = data.drop(columns=[\"target\"])\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    model_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\"\n",
    "    }\n",
    "\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    import pathlib\n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontainerSpec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageUri\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.metadata['containerSpec'] = {\n",
    "    'imageUri': \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google_cloud_pipeline_components'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compiler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from google_cloud_pipeline_components.v1.model import ModelUploadOp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;129;43m@pipeline\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcp-mlops-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPELINE_ROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mpipeline\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_data_url\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data/2021/2012-01.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_split_date\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2021-12-20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_op\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_split_date\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfrom\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mgoogle_cloud_pipeline_components\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01mexperimental\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01mcustom_job\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01mutils\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_custom_training_job_op_from_component\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/pipeline_context.py:65\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(func, name, description, pipeline_root, display_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[1;32m     63\u001b[0m     func\u001b[38;5;241m.\u001b[39mpipeline_root \u001b[38;5;241m=\u001b[39m pipeline_root\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomponent_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_graph_component_from_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_factory.py:654\u001b[0m, in \u001b[0;36mcreate_graph_component_from_func\u001b[0;34m(func, name, description, display_name)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    649\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m extract_component_interface(\n\u001b[1;32m    650\u001b[0m     func,\n\u001b[1;32m    651\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    652\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    653\u001b[0m )\n\u001b[0;32m--> 654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/graph_component.py:58\u001b[0m, in \u001b[0;36mGraphComponent.__init__\u001b[0;34m(self, component_spec, pipeline_func, display_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     50\u001b[0m         pipeline_channel\u001b[38;5;241m.\u001b[39mcreate_pipeline_channel(\n\u001b[1;32m     51\u001b[0m             name\u001b[38;5;241m=\u001b[39marg_name,\n\u001b[1;32m     52\u001b[0m             channel_type\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     53\u001b[0m             is_artifact_list\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mis_artifact_list,\n\u001b[1;32m     54\u001b[0m         ))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pipeline_context\u001b[38;5;241m.\u001b[39mPipeline(\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m---> 58\u001b[0m     pipeline_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is missing from pipeline.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(training_data_url, test_split_date)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;129m@pipeline\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcp-mlops-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_root\u001b[38;5;241m=\u001b[39mPIPELINE_ROOT)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline\u001b[39m(\n\u001b[1;32m      9\u001b[0m     training_data_url: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data/2021/2012-01.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     test_split_date: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-12-20\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ):\n\u001b[1;32m     12\u001b[0m     data_op \u001b[38;5;241m=\u001b[39m data_download(\n\u001b[1;32m     13\u001b[0m         data_url\u001b[38;5;241m=\u001b[39mtraining_data_url,\n\u001b[1;32m     14\u001b[0m         split_date\u001b[38;5;241m=\u001b[39mtest_split_date\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle_cloud_pipeline_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_job\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m         create_custom_training_job_op_from_component,\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     custom_job_distributed_training_op \u001b[38;5;241m=\u001b[39m create_custom_training_job_op_from_component(\n\u001b[1;32m     22\u001b[0m         model_train, replica_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     model_train_op \u001b[38;5;241m=\u001b[39m custom_job_distributed_training_op(\n\u001b[1;32m     26\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdata_op\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_train\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     27\u001b[0m         project\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[1;32m     28\u001b[0m         location\u001b[38;5;241m=\u001b[39mREGION,\n\u001b[1;32m     29\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google_cloud_pipeline_components'"
     ]
    }
   ],
   "source": [
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "# from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "\n",
    "@pipeline(name=\"gcp-mlops-v0\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    training_data_url: str = f\"gs://{BUCKET}/data/2021/2012-01.csv\",\n",
    "    test_split_date: str = \"2021-12-20\",\n",
    "):\n",
    "    data_op = data_download(\n",
    "        data_url=training_data_url,\n",
    "        split_date=test_split_date\n",
    "    )\n",
    "\n",
    "    from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "        create_custom_training_job_op_from_component,\n",
    "    )\n",
    "\n",
    "    custom_job_distributed_training_op = create_custom_training_job_op_from_component(\n",
    "        model_train, replica_count=1\n",
    "    )\n",
    "\n",
    "    model_train_op = custom_job_distributed_training_op(\n",
    "        dataset=data_op.outputs[\"dataset_train\"],\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "\n",
    "    model_evaluate_op = model_evaluate(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=model_train_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-model\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_evaluate_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"flight-delay-endpoint\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"flight-delay-model\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"gcp-mlops-v0.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipelines_intro_kfp.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
