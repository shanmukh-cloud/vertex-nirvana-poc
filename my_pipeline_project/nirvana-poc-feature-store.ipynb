{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Online and Batch predictions using Vertex AI Feature Store\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/sdk-feature-store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/sdk-feature-store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "    </td>\n",
    "    <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/feature_store/sdk-feature-store.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4aaea3bab5e"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook introduces Vertex AI Feature Store, a managed cloud service for machine learning engineers and data scientists to store, serve, manage and share machine learning features at a large scale.\n",
    "\n",
    "This notebook assumes that you understand basic Google Cloud concepts such as [Project](https://cloud.google.com/storage/docs/projects), [Storage](https://cloud.google.com/storage) and [Vertex AI](https://cloud.google.com/vertex-ai/docs). Some machine learning knowledge is also helpful but not required.\n",
    "\n",
    "Learn more about [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71779c8088bf"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to use `Vertex AI Feature Store` to import feature data, and to access the feature data for both online serving and offline tasks, such as training.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Feature Store`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create featurestore, entity type, and feature resources.\n",
    "- Import feature data into `Vertex AI Feature Store` resource.\n",
    "- Serve online prediction requests using the imported features.\n",
    "- Access imported features in offline jobs, such as training jobs.\n",
    "- Use streaming ingestion to ingest small amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55e01a856f57"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "This notebook uses a movie recommendation dataset as an example throughout all the sessions. The task is to train a model to predict if a user is going to watch a movie and serve this model online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Cloud BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3Jje0B5zglA"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MW_NeIHMzjoZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-datastore: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/google_cloud_datastore-1.15.5.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlWoVi7xz1TL"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CFS6OPNWz3KZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RMhe6650CyB"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7C_dgnR0L_l"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WbSe_XFH0NjL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"vertex-nirvana-poc\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybtwdOp40TVK"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oLUOopdB0UkU"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_ZkpZnv0a0b"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfsExLao0b49"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovUeYbbM0nmK"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_AmeEXr0pE1"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsl-OPfF0sUO"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOh0DLZP0vUI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qML_uytf0ymm"
   },
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVRl2Isi02ZG"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CtiQt7ST06f1"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://vertex-nirvana-poc\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = f\"gs://vertex-nirvana-poc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBr6o7cC1AEj"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4QWPo2V1BP0"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xuQ4jQTb1Jbc"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import Feature, Featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOqQGVoO1Kw-"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PWaQMlJ71N4e"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_HmF24mBHv9"
   },
   "source": [
    "## Terminology and concept\n",
    "\n",
    "### Featurestore data model\n",
    "\n",
    "Vertex AI Feature Store organizes data with the following 3 important hierarchical concepts:\n",
    "```\n",
    "Featurestore -> Entity type -> Feature\n",
    "```\n",
    "* **Featurestore**: The place to store your features\n",
    "* **Entity type**: Under a featurestore, an entity type describes an object to be modeled, real one or virtual one.\n",
    "* **Feature**: Under an entity type, a feature describes an attribute of the entity type\n",
    "\n",
    "The movie prediction example lets you create a featurestore called `movie_prediction`. This store has 2 entity types. `users` and `movies`. The `users` entity type has the `age`, `gender`, and `liked_genres` features. The `movies` entity type has the `titles`, `genres`, and `average rating` features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UvxYyGUimKw"
   },
   "source": [
    "## Create featurestore and define schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "from datetime import datetime\n",
    "def named_tuple_to_avro_fields(named_tuple):\n",
    "    fields = []\n",
    "    for field_name, field_type in named_tuple.__annotations__.items():\n",
    "        fields.append({\"name\": field_name, \"type\": map_to_avro_type(field_type)})\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flight(NamedTuple):\n",
    "    timestamp: Optional[datetime]\n",
    "    flight_number: str\n",
    "    origin_airport_id: str\n",
    "    is_cancelled: bool\n",
    "    departure_delay_minutes: float\n",
    "    arrival_delay_minutes: float\n",
    "    taxi_out_minutes: float\n",
    "    distance_miles: float\n",
    "\n",
    "class AirportFeatures(NamedTuple):\n",
    "    timestamp: Optional[datetime]\n",
    "    origin_airport_id: str\n",
    "    average_departure_delay: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delays_feature_store = aip.Featurestore(\n",
    "    \"flight_delays\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_delays_feature_store = aip.Featurestore.create(\n",
    "#     \"flight_delays\", online_store_fixed_node_count=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EntityType\n",
      "Create EntityType backing LRO: projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport/operations/8332450615410032640\n",
      "EntityType created. Resource name: projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport\n",
      "To use this EntityType in another session:\n",
      "entity_type = aiplatform.EntityType('projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport')\n"
     ]
    }
   ],
   "source": [
    "airport_entity_type = flight_delays_feature_store.create_entity_type(\n",
    "    entity_type_id=\"airport\",\n",
    "    description=\"Airport entity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Feature\n",
      "Create Feature backing LRO: projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport/features/average_departure_delay/operations/2660729834690314240\n",
      "Feature created. Resource name: projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport/features/average_departure_delay\n",
      "To use this Feature in another session:\n",
      "feature = aiplatform.Feature('projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport/features/average_departure_delay')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f83241c93c0> \n",
       "resource name: projects/665554893092/locations/us-central1/featurestores/flight_delays/entityTypes/airport/features/average_departure_delay"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_entity_type.create_feature(\n",
    "    feature_id=\"average_departure_delay\",\n",
    "    value_type=\"DOUBLE\",\n",
    "    description=\"Average departure delay for that airport, calculated every 4h with 1h rolling window\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport features   entity_id average_departure_delay\n",
      "0     13851                    None\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"distance_miles\": 481.0,\n",
    "    \"departure_delay_minutes\": -6.0,\n",
    "    \"taxi_out_minutes\": 13.0,\n",
    "    \"airport_id\": \"13851\",\n",
    "}\n",
    "\n",
    "aip.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "flight_delays_feature_store = aip.Featurestore(\n",
    "    \"flight_delays\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "airport_entity_type = flight_delays_feature_store.get_entity_type(\"airport\")\n",
    "\n",
    "features_df = airport_entity_type.read(\n",
    "    entity_ids=inputs[\"airport_id\"], feature_ids=[\"average_departure_delay\"]\n",
    ")\n",
    "\n",
    "print(\"Airport features\", features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aiplatform \u001b[38;5;28;01mas\u001b[39;00m aip\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Artifact,\n\u001b[1;32m      4\u001b[0m     Dataset,\n\u001b[1;32m      5\u001b[0m     Input,\n\u001b[1;32m      6\u001b[0m     Model,\n\u001b[1;32m      7\u001b[0m     Output,\n\u001b[1;32m      8\u001b[0m     ClassificationMetrics,\n\u001b[1;32m      9\u001b[0m     component,\n\u001b[1;32m     10\u001b[0m     pipeline,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compiler\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/__init__.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# compile-time only dependencies\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_KFP_RUNTIME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# make `from kfp import components` and `from kfp import dsl` valid;\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# related to namespace packaging issue\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components  \u001b[38;5;66;03m# noqa: keep unused import\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dsl  \u001b[38;5;66;03m# noqa: keep unused import\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client  \u001b[38;5;66;03m# noqa: keep unused import\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/components/__init__.py:27\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The Kubeflow Authors\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_component_from_file\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_component_from_url\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYamlComponent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m ]\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_yaml_utilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_component_from_file\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_yaml_utilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_component_from_text\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_yaml_utilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_component_from_url\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/components/load_yaml_utilities.py:18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"Functions for loading components from compiled YAML.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structures\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m yaml_component\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/__init__.py:230\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# compile-time only dependencies\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_KFP_RUNTIME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m component\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainer_component_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m container_component\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfor_loop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Collected\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m component_factory\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomponent\u001b[39m(func: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m               \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m     24\u001b[0m               base_image: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m               install_kfp_package: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m               kfp_package_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decorator for Python-function based components.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    A KFP component can either be a lightweight component or a containerized\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m            my_function_two_task = my_function_two(input=my_function_one_task.outputs)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_factory.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m container_component_artifact_channel\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m container_component_class\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph_component\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m placeholders\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/container_component_class.py:18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"Container-based component.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_component\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structures\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mContainerComponent\u001b[39;00m(base_component\u001b[38;5;241m.\u001b[39mBaseComponent):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/base_component.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline_task\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structures\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_utils\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/pipeline_task.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline_channel\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m placeholders\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structures\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_utils\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/dsl/structures.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_annotations\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_utils\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_spec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline_spec_pb2\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@dataclasses\u001b[39m\u001b[38;5;241m.\u001b[39mdataclass\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInputSpec\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kfp/pipeline_spec/pipeline_spec_pb2.py:38\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m status_pb2 \u001b[38;5;28;01mas\u001b[39;00m google_dot_rpc_dot_status__pb2\n\u001b[1;32m     19\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     20\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline_spec.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_pipelines\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[google_dot_protobuf_dot_duration__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,google_dot_protobuf_dot_struct__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,google_dot_rpc_dot_status__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n\u001b[1;32m     31\u001b[0m _PRIMITIVETYPE_PRIMITIVETYPEENUM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     32\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrimitiveTypeEnum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_pipelines.PrimitiveType.PrimitiveTypeEnum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     36\u001b[0m   create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m     37\u001b[0m   values\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 38\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumValueDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRIMITIVE_TYPE_UNSPECIFIED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINT\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[1;32m     48\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m     49\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOUBLE\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     50\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[1;32m     53\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m     54\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTRING\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     55\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[1;32m     58\u001b[0m   ],\n\u001b[1;32m     59\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\030\u001b[39;00m\u001b[38;5;130;01m\\001\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     61\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6725\u001b[39m,\n\u001b[1;32m     62\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6813\u001b[39m,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m _sym_db\u001b[38;5;241m.\u001b[39mRegisterEnumDescriptor(_PRIMITIVETYPE_PRIMITIVETYPEENUM)\n\u001b[1;32m     66\u001b[0m _PARAMETERTYPE_PARAMETERTYPEENUM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameterTypeEnum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_pipelines.ParameterType.ParameterTypeEnum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7003\u001b[39m,\n\u001b[1;32m    118\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/protobuf/descriptor.py:796\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, index, number,\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    795\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 796\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m   \u001b[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[1;32m    798\u001b[0m   \u001b[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[1;32m    799\u001b[0m   \u001b[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[1;32m    800\u001b[0m   \u001b[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[1;32m    801\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    pipeline,\n",
    ")\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cdct_Lm7x2I_"
   },
   "outputs": [],
   "source": [
    "FEATURESTORE_ID = \"movie_prediction_unique\"\n",
    "INPUT_CSV_FILE = \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/movie_prediction.csv\"\n",
    "ONLINE_STORE_FIXED_NODE_COUNT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQBIv3ZL3A0"
   },
   "source": [
    "### Create featurestore\n",
    "\n",
    "The method to create a featurestore returns a\n",
    "[long-running operation](https://google.aip.dev/151) (LRO). An LRO starts an asynchronous job. LROs are returned for other API\n",
    "methods too, such as updating or deleting a featurestore. Running the code cell creates a featurestore and print the process log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D6uIWQeoBSr8"
   },
   "outputs": [],
   "source": [
    "fs = Featurestore.create(\n",
    "    featurestore_id=FEATURESTORE_ID,\n",
    "    online_store_fixed_node_count=ONLINE_STORE_FIXED_NODE_COUNT,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ag8pCQ7rNjVf"
   },
   "source": [
    "Use the following function call to retrieve a featurestore and check that it has been created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eKhD4q8rXjmM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique\"\n",
      "create_time {\n",
      "  seconds: 1696443936\n",
      "  nanos: 378213000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1696443936\n",
      "  nanos: 557531000\n",
      "}\n",
      "etag: \"AMEw9yM5eLsZ4vT8yXHfsuk7I7SUYXyxEslbOKb-RfHupa2oWDPuBap0OFilMkmrmjqs\"\n",
      "online_serving_config {\n",
      "  fixed_node_count: 1\n",
      "}\n",
      "state: STABLE\n",
      "online_storage_ttl_days: 4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs = Featurestore(\n",
    "    featurestore_name=FEATURESTORE_ID,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "print(fs.gca_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpmJq75zXjmT"
   },
   "source": [
    "### Create entity Type\n",
    "\n",
    "Entity types can be created within the `Featurestore` class. Below, create the `users` and `movies` entity types. A process log is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU0oXvINBgPV"
   },
   "outputs": [],
   "source": [
    "# Create the `users` entity type\n",
    "users_entity_type = fs.create_entity_type(\n",
    "    entity_type_id=\"users\",\n",
    "    description=\"Users entity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPCGFznrFwFy"
   },
   "outputs": [],
   "source": [
    "# Create the `movies` entity type\n",
    "movies_entity_type = fs.create_entity_type(\n",
    "    entity_type_id=\"movies\",\n",
    "    description=\"Movies entity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0TS9i5SJnkt"
   },
   "source": [
    "To retrieve an entity type or check that it has been created use the [get_entity_type](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/featurestore/featurestore.py#L106) or [list_entity_types](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/featurestore/featurestore.py#L278) methods on the Featurestore object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5DHcAyoFJmq9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f432d04b5e0> \n",
      "resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users\n",
      "<google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f42fe286b00> \n",
      "resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies\n"
     ]
    }
   ],
   "source": [
    "users_entity_type = fs.get_entity_type(entity_type_id=\"users\")\n",
    "movies_entity_type = fs.get_entity_type(entity_type_id=\"movies\")\n",
    "print(users_entity_type)\n",
    "print(movies_entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EWMzDNFT5qf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f4316900940> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users,\n",
       " <google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f43169020b0> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.list_entity_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJW4q-0jO2Xf"
   },
   "source": [
    "### Create feature\n",
    "You can create features within each entity type. Use the `create_feature` method to add features to the `users` and `movies` entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PvjwT84iVSps"
   },
   "outputs": [],
   "source": [
    "# To create one feature at a time, use:\n",
    "users_feature_age = users_entity_type.create_feature(\n",
    "    feature_id=\"age\",\n",
    "    value_type=\"INT64\",\n",
    "    description=\"User age\",\n",
    ")\n",
    "\n",
    "users_feature_gender = users_entity_type.create_feature(\n",
    "    feature_id=\"gender\",\n",
    "    value_type=\"STRING\",\n",
    "    description=\"User gender\",\n",
    ")\n",
    "\n",
    "users_feature_liked_genres = users_entity_type.create_feature(\n",
    "    feature_id=\"liked_genres\",\n",
    "    value_type=\"STRING_ARRAY\",\n",
    "    description=\"An array of genres this user liked\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ9-AyFYBvcX"
   },
   "source": [
    "Use the [`list_features`](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/featurestore/entity_type.py#L349) method to list all the features of a given entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S5Lr8uXk1xD0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fe1e6c50> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/features/age,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f43164d6f20> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/features/gender,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fe294e20> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/features/liked_genres]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_entity_type.list_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "llTT9_Dgbac2"
   },
   "outputs": [],
   "source": [
    "movies_feature_configs = {\n",
    "    \"title\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The title of the movie\",\n",
    "    },\n",
    "    \"genres\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The genre of the movie\",\n",
    "    },\n",
    "    \"average_rating\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"The average rating for the movie, range is [1.0-5.0]\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YhfOKJL_BvuM"
   },
   "outputs": [],
   "source": [
    "movie_features = movies_entity_type.batch_create_features(\n",
    "    feature_configs=movies_feature_configs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUVOzrAb1AFX"
   },
   "source": [
    "## Search created features\n",
    "\n",
    "While the `list_features` method lets you view all features for the same entity type,\n",
    "the [`search`](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/featurestore/feature.py#L352) method in the `Feature` class searches across all featurestores and entity types in a given location (such as `us-central1`) and returns a list of features. This lets you discover features created by someone else.\n",
    "\n",
    "You can query based on feature properties including feature ID, entity type ID, and feature description. You can also limit results by filtering based on a specific featurestore, feature value type, and/or label. Some search examples are shown below.\n",
    "\n",
    "**Example of using the `search` method**\n",
    "\n",
    "Use the following code snippet to search for all features within a feature store:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OAiv8p8LMdLJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fe0d81c0> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies/features/average_rating,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fd6d9990> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies/features/genres,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fd6da3e0> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies/features/title,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fd6d9960> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/features/age,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fd6da6e0> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/features/gender,\n",
       " <google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f42fd6d9e70> \n",
       " resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/features/liked_genres]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_features = Feature.search(query=\"featurestore_id={}\".format(FEATURESTORE_ID))\n",
    "my_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcxsiBUiIyvE"
   },
   "source": [
    "Now, narrow down the search to features that are of type `DOUBLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a9ovJSyEI4OZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies/features/average_rating\"\n",
       "description: \"The average rating for the movie, range is [1.0-5.0]\"\n",
       "value_type: DOUBLE\n",
       "create_time {\n",
       "  seconds: 1696444943\n",
       "  nanos: 581425000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1696444943\n",
       "  nanos: 581426000\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_features = Feature.search(\n",
    "    query=\"value_type=DOUBLE AND featurestore_id={}\".format(FEATURESTORE_ID)\n",
    ")\n",
    "double_features[0].gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wtr9tvH6JAOY"
   },
   "source": [
    "Or, limit the search results to features with specific keywords in their ID and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3G1mNV1uJFBC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies/features/title\"\n",
       "description: \"The title of the movie\"\n",
       "value_type: STRING\n",
       "create_time {\n",
       "  seconds: 1696444943\n",
       "  nanos: 581215000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1696444943\n",
       "  nanos: 581216000\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_features = Feature.search(\n",
    "    query=\"feature_id:title AND value_type=STRING AND featurestore_id={}\".format(\n",
    "        FEATURESTORE_ID\n",
    "    )\n",
    ")\n",
    "title_features[0].gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3n5XdK8Xjmw"
   },
   "source": [
    "## Import feature values\n",
    "\n",
    "You need to import feature values before you can use them for online or offline serving. In this step, you learn how to import feature values by ingesting the values from GCS (Google Cloud Storage). You can also import feature values from BigQuery or a pandas dataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlqJ-QdTcs6W"
   },
   "source": [
    "### Source data format and layout\n",
    "\n",
    "BigQuery table/Avro/CSV are supported as input data types. No matter what format you are using, each imported entity *must* have an ID. Each entity can *optionally* have a timestamp, specifying when the feature values are generated. This notebook uses Avro as an input, located at this public [bucket](https://console.cloud.google.com/storage/browser/cloud-samples-data-us-central1/vertex-ai/feature-store/datasets). The Avro schemas are as follows:\n",
    "\n",
    "**For the `users` entity**:\n",
    "```\n",
    "schema = {\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "      {\n",
    "       \"name\":\"user_id\",\n",
    "       \"type\":[\"null\",\"string\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"age\",\n",
    "       \"type\":[\"null\",\"long\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"gender\",\n",
    "       \"type\":[\"null\",\"string\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"liked_genres\",\n",
    "       \"type\":{\"type\":\"array\",\"items\":\"string\"}\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"update_time\",\n",
    "       \"type\":[\"null\",{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}]\n",
    "      },\n",
    "  ]\n",
    " }\n",
    "```\n",
    "\n",
    "**For the `movies` entity**:\n",
    "```\n",
    "schema = {\n",
    " \"type\": \"record\",\n",
    " \"name\": \"Movie\",\n",
    " \"fields\": [\n",
    "     {\n",
    "      \"name\":\"movie_id\",\n",
    "      \"type\":[\"null\",\"string\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"average_rating\",\n",
    "      \"type\":[\"null\",\"double\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"title\",\n",
    "      \"type\":[\"null\",\"string\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"genres\",\n",
    "      \"type\":[\"null\",\"string\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"update_time\",\n",
    "      \"type\":[\"null\",{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}]\n",
    "     },\n",
    " ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7DyDa6chbJx"
   },
   "source": [
    "### Import feature values for `users` entity type\n",
    "\n",
    "When importing, specify the following in your request:\n",
    "\n",
    "*   IDs of the features to import\n",
    "*   Data source URI\n",
    "*   Data source format: BigQuery Table/Avro/CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dS8QbDPfkSEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'gender', 'liked_genres']\n"
     ]
    }
   ],
   "source": [
    "USERS_FEATURES_IDS = [feature.name for feature in users_entity_type.list_features()]\n",
    "USERS_FEATURE_TIME = \"update_time\"\n",
    "USERS_ENTITY_ID_FIELD = \"user_id\"\n",
    "USERS_GCS_SOURCE_URI = (\n",
    "    \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/users.avro\"\n",
    ")\n",
    "GCS_SOURCE_TYPE = \"avro\"\n",
    "WORKER_COUNT = 1\n",
    "print(USERS_FEATURES_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x86Dtg0Vu-uf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing EntityType feature values: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f432d04b5e0> \n",
       "resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import EntityType feature values backing LRO: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users/operations/1794772068839981056\n"
     ]
    }
   ],
   "source": [
    "users_entity_type.ingest_from_gcs(\n",
    "    feature_ids=USERS_FEATURES_IDS,\n",
    "    feature_time=USERS_FEATURE_TIME,\n",
    "    entity_id_field=USERS_ENTITY_ID_FIELD,\n",
    "    gcs_source_uris=USERS_GCS_SOURCE_URI,\n",
    "    gcs_source_type=GCS_SOURCE_TYPE,\n",
    "    worker_count=WORKER_COUNT,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laXdJPIqkLJO"
   },
   "source": [
    "### Import feature values for `movies` entity type\n",
    "\n",
    "Similarly, import feature values for the `movies` entity type into the featurestore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-c_CHjd11AD0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genres', 'average_rating', 'title']\n"
     ]
    }
   ],
   "source": [
    "MOVIES_FEATURES_IDS = [feature.name for feature in movies_entity_type.list_features()]\n",
    "MOVIES_FEATURE_TIME = \"update_time\"\n",
    "MOVIES_ENTITY_ID_FIELD = \"movie_id\"\n",
    "MOVIES_GCS_SOURCE_URI = (\n",
    "    \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/movies.avro\"\n",
    ")\n",
    "GCS_SOURCE_TYPE = \"avro\"\n",
    "WORKER_COUNT = 1\n",
    "print(MOVIES_FEATURES_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Q4mMBRTc1Xpx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing EntityType feature values: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.entity_type.EntityType object at 0x7f42fe286b00> \n",
       "resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import EntityType feature values backing LRO: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies/operations/4573493038927577088\n"
     ]
    }
   ],
   "source": [
    "movies_entity_type.ingest_from_gcs(\n",
    "    feature_ids=MOVIES_FEATURES_IDS,\n",
    "    feature_time=MOVIES_FEATURE_TIME,\n",
    "    entity_id_field=MOVIES_ENTITY_ID_FIELD,\n",
    "    gcs_source_uris=MOVIES_GCS_SOURCE_URI,\n",
    "    gcs_source_type=GCS_SOURCE_TYPE,\n",
    "    worker_count=WORKER_COUNT,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TdxPYdDXjnA"
   },
   "source": [
    "## Get online predictions from your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezJIMyU-XjnB"
   },
   "source": [
    "[Online serving](https://cloud.google.com/vertex-ai/docs/featurestore/serving-online)\n",
    "lets you serve feature values for small batches of entities. It's designed for latency-sensitive services, such as online model prediction. For example, for a movie service, you might want to quickly show movies that the current user would most likely watch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foNB0D2aw37c"
   },
   "source": [
    "### Read one entity per request\n",
    "\n",
    "With the Python SDK, it's easy to read feature values of one entity. By default, the SDK returns the latest value of each feature, that is, the feature values with the most recent timestamps.\n",
    "\n",
    "To read feature values, specify the entity type ID and features to read. By default all the features of an entity type are selected. The output response displays the selected entity type ID and the selected feature values as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "084G80L_DWwd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityType feature values imported. Resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/users\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>liked_genres</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bob</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_id         liked_genres  age gender\n",
       "0       bob  [Action, Adventure]   35   Male"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityType feature values imported. Resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/entityTypes/movies\n"
     ]
    }
   ],
   "source": [
    "users_entity_type.read(entity_ids=\"bob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qrR-SY3i58rh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie_01</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_id                     title\n",
       "0  movie_01  The Shawshank Redemption"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_entity_type.read(entity_ids=\"movie_01\", feature_ids=\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYk83Zt9xF8m"
   },
   "source": [
    "### Read multiple entities per request\n",
    "\n",
    "To read feature values from multiple entities, specify the different entity type IDs. By default, all the features of an entity type are selected. Note that fetching only a small number of entities is recommended when using this SDK due to its latency-sensitive nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BIJFcIIHULOd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>liked_genres</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alice</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>55</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_id         liked_genres  age  gender\n",
       "0     alice              [Drama]   55  Female\n",
       "1       bob  [Action, Adventure]   35    Male"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_entity_type.read(entity_ids=[\"bob\", \"alice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DtL0uy_0Dcca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>title, genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie_02</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie_03</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie_04</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_id title, genres\n",
       "0  movie_02          None\n",
       "1  movie_03          None\n",
       "2  movie_04          None"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_entity_type.read(\n",
    "    entity_ids=[\"movie_02\", \"movie_03\", \"movie_04\"], feature_ids=[\"title, genres\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sds42j8ZsCzS"
   },
   "source": [
    "Now that you have learned how to fetch imported feature values for online serving, the next step is learning how to use imported feature values for offline use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpvhPAYxD-Ml"
   },
   "source": [
    "## Get batch predictions from your model\n",
    "\n",
    "Batch serving is used to fetch a large batch of feature values for high-throughput, and is typically used for training a model or batch prediction. In this section, you learn how to prepare for training examples by using the Featurestore's batch serve function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPzAGvqJHh3B"
   },
   "source": [
    "### Use case\n",
    "\n",
    "**The task** is to prepare a training dataset to train a model, which predicts if a given user is going to watch a movie. To achieve this, you need 2 sets of input:\n",
    "\n",
    "*   Features: you already imported into the featurestore.\n",
    "*   Labels: the ground-truth data recorded that user X has watched movie Y.\n",
    "\n",
    "\n",
    "To be more specific, the ground-truth observation is described in Table 1 and the desired training dataset is described in Table 2. Each row in Table 2 is a result of joining the imported feature values from Vertex AI Feature Store according to the entity IDs and timestamps in Table 1. In this example,  the `age`, `gender` and `liked_genres` features from `users` and\n",
    "the `titles`, `genres` and `average_rating` features from `movies` are chosen to train the model. Note that only positive examples are shown in these 2 tables, that is, you can imagine there is a label column whose values are all `True`.\n",
    "\n",
    "[`batch_serve_to_bq`](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/featurestore/featurestore.py#L770) takes Table 1 as\n",
    "input, joins all required feature values from the featurestore, and returns Table 2 for training.\n",
    "\n",
    "<h4 align=\"center\">Table 1. Ground-truth data</h4>\n",
    "\n",
    "users | movies | timestamp\n",
    "----- | -------- | --------------------\n",
    "alice  | Cinema Paradiso     | 2019-11-01T00:00:00Z\n",
    "bob  | The Shining     | 2019-11-15T18:09:43Z\n",
    "...   | ...      | ...\n",
    "\n",
    "\n",
    "<h4 align=\"center\">Table 2. Expected training data generated by using batch serve</h4>\n",
    "\n",
    "timestamp            | entity_type_users | age | gender | liked_genres | entity_type_movies | title | genre | average_rating\n",
    "-------------------- | ----------------- | --------------- | ---------------- | -------------------- | - | -------- | --------- | -----\n",
    "2019-11-01T00:00:00Z | bob              | 35        | M                | [Action, Crime]      | movie_02            | The Shining | Horror | 4.8\n",
    "2019-11-01T00:00:00Z | alice             | 55        | F                | [Drama, Comedy]      | movie_03          | Cinema Paradiso | Romance | 4.5 |\n",
    "... | ... | ... | ... | ... | ... | ... | ... | ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKwWGg2i4a4G"
   },
   "source": [
    "#### Why timestamp?\n",
    "\n",
    "Note that there is a `timestamp` column in Table 2 to indicate the time when the ground-truth was observed. This is to avoid data inconsistency.\n",
    "\n",
    "For example, the 2nd row of Table 2 indicates that user `alice` watched movie `Cinema Paradiso` on `2019-11-01T00:00:00Z`. The featurestore keeps feature values for all timestamps but fetches feature values *only* at the given timestamp during batch serving. On that day, Alice might have been 54 years old, but now Alice might be 56; featurestore returns `age=54` as Alice's age, instead of `age=56`, because that is the value of the feature at the observation time. Similarly, other features might be time-variant as well, such as `liked_genres`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAL9Y4VTOLT1"
   },
   "source": [
    "### Create BigQuery dataset for output\n",
    "\n",
    "You need a BigQuery dataset to host the output data in `us-central1`. Input the name of the dataset you want to create and specify the name of the table you want to store the output created later. These are used in the next section.\n",
    "\n",
    "**Make sure that the table name does NOT already exist**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4Oc-jrd6Ow7N"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "QOaGi2PrEAwA"
   },
   "outputs": [],
   "source": [
    "# Output dataset\n",
    "DESTINATION_DATA_SET = \"movie_predictions\"  # @param {type:\"string\"}\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DESTINATION_DATA_SET = \"{prefix}_{timestamp}\".format(\n",
    "    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n",
    ")\n",
    "\n",
    "# Output table. Make sure that the table does NOT already exist; the BatchReadFeatureValues API cannot overwrite an existing table\n",
    "DESTINATION_TABLE_NAME = \"training_data\"  # @param {type:\"string\"}\n",
    "\n",
    "DESTINATION_PATTERN = \"bq://{project}.{dataset}.{table}\"\n",
    "DESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=DESTINATION_TABLE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "RKhmymT-O0vy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset vertex-nirvana-poc.movie_predictions_20231009103537\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset_id = \"{}.{}\".format(client.project, DESTINATION_DATA_SET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = REGION\n",
    "dataset = client.create_dataset(dataset)\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8dLJ9nuDFgI"
   },
   "source": [
    "### Batch read feature values\n",
    "\n",
    "Assemble the request which specifies the following info:\n",
    "\n",
    "*   Where is the label data, i.e., Table 1.\n",
    "*   Which features are read, i.e., the column names in Table 2.\n",
    "\n",
    "The output is stored in the BigQuery table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "0p752VDHSmFn"
   },
   "outputs": [],
   "source": [
    "SERVING_FEATURE_IDS = {\n",
    "    # to choose all the features use 'entity_type_id: ['*']'\n",
    "    \"users\": [\"age\", \"gender\", \"liked_genres\"],\n",
    "    \"movies\": [\"title\", \"average_rating\", \"genres\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "VsAaLGyHRYll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving Featurestore feature values: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique\n",
      "Serve Featurestore feature values backing LRO: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique/operations/2952197173074198528\n",
      "Featurestore feature values served. Resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.featurestore.Featurestore object at 0x7f43164e82e0> \n",
       "resource name: projects/665554893092/locations/us-central1/featurestores/movie_prediction_unique"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.batch_serve_to_bq(\n",
    "    bq_destination_output_uri=DESTINATION_TABLE_URI,\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=INPUT_CSV_FILE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RAdjahHQ93J"
   },
   "source": [
    "After the LRO finishes, you should be able to see the result in the [BigQuery console](https://console.cloud.google.com/bigquery), as a new table under the BigQuery dataset created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7190f3c8b625"
   },
   "source": [
    "## Streaming ingestion\n",
    "\n",
    "Streaming ingestion is currently public preview.\n",
    "\n",
    "Streaming ingestion lets you make real-time updates to feature values. While batch import is suitable for importing a large volume of data with high latency, streaming ingestion is suitable for ingesting small amount of data with low latency. The written data becomes available to read using batch export and online serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "560e835c93db"
   },
   "outputs": [],
   "source": [
    "# Since streaming ingestion is public preview, the feature is available in aiplatform_v1beta1.\n",
    "from google.cloud.aiplatform_v1beta1 import (\n",
    "    FeaturestoreOnlineServingServiceClient, FeaturestoreServiceClient)\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_online_service as featurestore_online_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import types as types_pb2\n",
    "\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "# Create client connection\n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "data_client = FeaturestoreOnlineServingServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "f53a06c9ab5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call `write_feature_values` to ingest data to `users` entity type.\n",
    "data_client.write_feature_values(\n",
    "    entity_type=admin_client.entity_type_path(\n",
    "        PROJECT_ID, REGION, FEATURESTORE_ID, \"users\"\n",
    "    ),\n",
    "    payloads=[\n",
    "        featurestore_online_service_pb2.WriteFeatureValuesPayload(\n",
    "            entity_id=\"1305\",\n",
    "            feature_values={\n",
    "                \"age\": featurestore_online_service_pb2.FeatureValue(int64_value=34),\n",
    "                \"gender\": featurestore_online_service_pb2.FeatureValue(\n",
    "                    string_value=\"female\"\n",
    "                ),\n",
    "                \"liked_genres\": featurestore_online_service_pb2.FeatureValue(\n",
    "                    string_array_value=types_pb2.StringArray(values=[\"drama\", \"action\"])\n",
    "                ),\n",
    "            },\n",
    "        ),\n",
    "        featurestore_online_service_pb2.WriteFeatureValuesPayload(\n",
    "            entity_id=\"1306\",\n",
    "            feature_values={\n",
    "                \"age\": featurestore_online_service_pb2.FeatureValue(int64_value=50),\n",
    "                \"gender\": featurestore_online_service_pb2.FeatureValue(\n",
    "                    string_value=\"male\"\n",
    "                ),\n",
    "                \"liked_genres\": featurestore_online_service_pb2.FeatureValue(\n",
    "                    string_array_value=types_pb2.StringArray(\n",
    "                        values=[\"suspense\", \"comedy\"]\n",
    "                    )\n",
    "                ),\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "700a9f1ebd19"
   },
   "source": [
    "Upon successful completion, the `write_feature_values` API returns an empty response.\n",
    "Similarly, ingest data to the `movies` entity type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "You can also keep the project, but delete the featurestore and the BigQuery dataset by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBTNfN8vxz4x"
   },
   "outputs": [],
   "source": [
    "# Delete Featurestore\n",
    "fs.delete(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzkD2SWJ2gIK"
   },
   "outputs": [],
   "source": [
    "# Delete BigQuery dataset\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "client.delete_dataset(\n",
    "    DESTINATION_DATA_SET, delete_contents=True, not_found_ok=True\n",
    ")  # Make an API request.\n",
    "\n",
    "print(\"Deleted dataset '{}'.\".format(DESTINATION_DATA_SET))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BlqJ-QdTcs6W",
    "hKwWGg2i4a4G"
   ],
   "name": "sdk-feature-store.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
